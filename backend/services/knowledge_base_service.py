"""
Knowledge Base Service - X·ª≠ l√Ω logic nghi·ªáp v·ª• cho knowledge base

Service n√†y ch·ª©a:
- X·ª≠ l√Ω upload file PDF
- Tr√≠ch xu·∫•t text t·ª´ PDF
- Qu·∫£n l√Ω metadata
- Validation file
"""

import os
import hashlib
import json
import uuid
from datetime import datetime
from werkzeug.utils import secure_filename
import PyPDF2
import chromadb
from chromadb.config import Settings
import re

class KnowledgeBaseService:
    """
    Service x·ª≠ l√Ω c√°c thao t√°c li√™n quan ƒë·∫øn knowledge base
    """
    
    def __init__(self, upload_folder='uploads', chroma_db_path='./chroma_db'):
        """
        Kh·ªüi t·∫°o service
        
        Args:
            upload_folder: Th∆∞ m·ª•c l∆∞u file upload
            chroma_db_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn ChromaDB
        """
        self.upload_folder = upload_folder
        self.chroma_db_path = chroma_db_path
        self.allowed_extensions = {'pdf'}
        self.max_file_size = 10 * 1024 * 1024  # 10MB
        
        # T·∫°o th∆∞ m·ª•c uploads n·∫øu ch∆∞a t·ªìn t·∫°i
        if not os.path.exists(self.upload_folder):
            os.makedirs(self.upload_folder)
        
        # Kh·ªüi t·∫°o ChromaDB client v√† collection
        self._init_chroma_db()
    
    def is_allowed_file(self, filename):
        """
        Ki·ªÉm tra file c√≥ ƒë∆∞·ª£c ph√©p upload kh√¥ng
        
        Args:
            filename: T√™n file
            
        Returns:
            bool: True n·∫øu file ƒë∆∞·ª£c ph√©p upload
        """
        return '.' in filename and \
               filename.rsplit('.', 1)[1].lower() in self.allowed_extensions
    
    def validate_file_size(self, file):
        """
        Ki·ªÉm tra k√≠ch th∆∞·ªõc file
        
        Args:
            file: File object t·ª´ request
            
        Returns:
            tuple: (is_valid, file_size, error_message)
        """
        try:
            file.seek(0, 2)  # Seek to end of file
            file_size = file.tell()
            file.seek(0)  # Reset to beginning
            
            if file_size > self.max_file_size:
                return False, file_size, f"File size must be less than {self.max_file_size // (1024*1024)}MB"
            
            return True, file_size, None
            
        except Exception as e:
            return False, 0, f"Error checking file size: {str(e)}"
    
    def extract_text_from_pdf(self, file_path):
        """
        Tr√≠ch xu·∫•t text t·ª´ file PDF
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file PDF
            
        Returns:
            tuple: (success, text, pages_count, error_message)
        """
        try:
            text = ""
            with open(file_path, 'rb') as file:
                pdf_reader = PyPDF2.PdfReader(file)
                pages_count = len(pdf_reader.pages)
                
                for page in pdf_reader.pages:
                    text += page.extract_text() + "\n"
            
            return True, text, pages_count, None
            
        except Exception as e:
            return False, "", 0, f"Error extracting text from PDF: {str(e)}"
    
    def calculate_file_hash(self, file_path):
        """
        T√≠nh hash MD5 c·ªßa file
        
        Args:
            file_path: ƒê∆∞·ªùng d·∫´n ƒë·∫øn file
            
        Returns:
            str: Hash MD5 c·ªßa file
        """
        hash_md5 = hashlib.md5()
        with open(file_path, "rb") as f:
            for chunk in iter(lambda: f.read(4096), b""):
                hash_md5.update(chunk)
        return hash_md5.hexdigest()
    
    def generate_unique_filename(self, filename):
        """
        T·∫°o t√™n file unique v·ªõi timestamp
        
        Args:
            filename: T√™n file g·ªëc
            
        Returns:
            tuple: (unique_filename, timestamp)
        """
        secure_name = secure_filename(filename)
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        unique_filename = f"{timestamp}_{secure_name}"
        return unique_filename, timestamp
    
    def save_file_metadata(self, file_id, metadata):
        """
        L∆∞u metadata c·ªßa file
        
        Args:
            file_id: ID c·ªßa file
            metadata: Dictionary ch·ª©a metadata
            
        Returns:
            tuple: (success, metadata_path, error_message)
        """
        try:
            metadata_path = os.path.join(self.upload_folder, f"{file_id}_metadata.json")
            with open(metadata_path, 'w', encoding='utf-8') as f:
                json.dump(metadata, f, ensure_ascii=False, indent=2)
            
            return True, metadata_path, None
            
        except Exception as e:
            return False, "", f"Error saving metadata: {str(e)}"
    
    def save_extracted_text(self, file_id, text):
        """
        L∆∞u text ƒë√£ tr√≠ch xu·∫•t v√†o file ri√™ng
        
        Args:
            file_id: ID c·ªßa file
            text: Text ƒë√£ tr√≠ch xu·∫•t
            
        Returns:
            tuple: (success, text_path, error_message)
        """
        try:
            text_path = os.path.join(self.upload_folder, f"{file_id}_text.txt")
            with open(text_path, 'w', encoding='utf-8') as f:
                f.write(text)
            
            return True, text_path, None
            
        except Exception as e:
            return False, "", f"Error saving extracted text: {str(e)}"
    
    def _init_chroma_db(self):
        """
        Kh·ªüi t·∫°o ChromaDB client v√† collection
        
        T·∫°o connection ƒë·∫øn ChromaDB v√† collection ƒë·ªÉ l∆∞u tr·ªØ vector embeddings
        c·ªßa text ƒë√£ tr√≠ch xu·∫•t t·ª´ PDF files
        """
        try:
            # T·∫°o th∆∞ m·ª•c ChromaDB n·∫øu ch∆∞a t·ªìn t·∫°i
            if not os.path.exists(self.chroma_db_path):
                os.makedirs(self.chroma_db_path)
            
            # Kh·ªüi t·∫°o ChromaDB client v·ªõi persistent storage
            self.chroma_client = chromadb.PersistentClient(path=self.chroma_db_path)
            
            # T·∫°o ho·∫∑c l·∫•y collection cho knowledge base
            # Collection n√†y s·∫Ω l∆∞u tr·ªØ text chunks v√† metadata
            self.collection = self.chroma_client.get_or_create_collection(
                name="knowledge_base",
                metadata={"description": "PDF document knowledge base with text chunks"}
            )
            
            print(f"‚úÖ ChromaDB initialized successfully at: {self.chroma_db_path}")
            
        except Exception as e:
            print(f"‚ùå Error initializing ChromaDB: {str(e)}")
            self.chroma_client = None
            self.collection = None
    
    def _split_text_into_chunks(self, text, chunk_size=1000, overlap=200):
        """
        Chia text th√†nh c√°c chunks nh·ªè ƒë·ªÉ l∆∞u v√†o vector database
        C·∫£i thi·ªán ƒë·ªÉ x·ª≠ l√Ω ti·∫øng Vi·ªát t·ªët h∆°n
        
        Args:
            text: Text c·∫ßn chia (ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh)
            chunk_size: K√≠ch th∆∞·ªõc m·ªói chunk (s·ªë k√Ω t·ª±)
            overlap: S·ªë k√Ω t·ª± overlap gi·ªØa c√°c chunk
            
        Returns:
            list: Danh s√°ch c√°c text chunks
        """
        # L√†m s·∫°ch text: lo·∫°i b·ªè k√Ω t·ª± xu·ªëng d√≤ng th·ª´a v√† kho·∫£ng tr·∫Øng
        cleaned_text = re.sub(r'\n+', '\n', text.strip())
        cleaned_text = re.sub(r'\s+', ' ', cleaned_text)
        
        if len(cleaned_text) <= chunk_size:
            return [cleaned_text]
        
        chunks = []
        start = 0
        
        while start < len(cleaned_text):
            # T√≠nh v·ªã tr√≠ k·∫øt th√∫c chunk
            end = start + chunk_size
            
            # N·∫øu ch∆∞a ƒë·∫øn cu·ªëi text, t√¨m ƒëi·ªÉm ng·∫Øt t·ª± nhi√™n (c√¢u, ƒëo·∫°n)
            if end < len(cleaned_text):
                # T√¨m ƒëi·ªÉm ng·∫Øt g·∫ßn nh·∫•t v·ªõi ∆∞u ti√™n cao cho ti·∫øng Vi·ªát
                # Th√™m c√°c d·∫•u c√¢u ti·∫øng Vi·ªát
                break_chars = ['. ', '\n', '! ', '? ', '; ', ': ', '.\n', '!\n', '?\n']
                
                best_break = -1
                for break_char in break_chars:
                    break_pos = cleaned_text.rfind(break_char, start, end)
                    if break_pos != -1:
                        best_break = break_pos + len(break_char)
                        break
                
                # N·∫øu kh√¥ng t√¨m th·∫•y d·∫•u c√¢u, t√¨m kho·∫£ng tr·∫Øng g·∫ßn cu·ªëi chunk
                if best_break == -1:
                    space_pos = cleaned_text.rfind(' ', start, end)
                    if space_pos != -1 and space_pos > start + chunk_size * 0.7:
                        best_break = space_pos + 1
                
                if best_break != -1:
                    end = best_break
            
            # L·∫•y chunk text
            chunk = cleaned_text[start:end].strip()
            if chunk:  # Ch·ªâ th√™m chunk kh√¥ng r·ªóng
                chunks.append(chunk)
            
            # Di chuy·ªÉn start position v·ªõi overlap
            start = max(start + 1, end - overlap)
            
            # Tr√°nh infinite loop
            if start >= len(cleaned_text):
                break
        
        return chunks
    
    def save_to_vector_db(self, file_id, title, description, extracted_text, metadata):
        """
        L∆∞u text ƒë√£ tr√≠ch xu·∫•t v√†o ChromaDB vector database
        
        Args:
            file_id: UUID c·ªßa file
            title: Ti√™u ƒë·ªÅ t√†i li·ªáu
            description: M√¥ t·∫£ t√†i li·ªáu  
            extracted_text: Text ƒë√£ tr√≠ch xu·∫•t t·ª´ PDF
            metadata: Metadata c·ªßa file
            
        Returns:
            tuple: (success, chunks_count, error_message)
        """
        try:
            # Ki·ªÉm tra ChromaDB ƒë√£ ƒë∆∞·ª£c kh·ªüi t·∫°o ch∆∞a
            if not self.collection:
                return False, 0, "ChromaDB not initialized"
            
            # Chia text th√†nh c√°c chunks nh·ªè
            text_chunks = self._split_text_into_chunks(extracted_text)
            
            if not text_chunks:
                return False, 0, "No text chunks to save"
            
            # Chu·∫©n b·ªã d·ªØ li·ªáu cho ChromaDB
            chunk_ids = []
            chunk_documents = []
            chunk_metadatas = []
            
            for i, chunk in enumerate(text_chunks):
                # T·∫°o unique ID cho m·ªói chunk (file_id + chunk_index)
                chunk_id = f"{file_id}_chunk_{i}"
                
                # Ph√°t hi·ªán ng√¥n ng·ªØ ch√≠nh c·ªßa chunk
                language = self._detect_language(chunk)
                
                # Chu·∫©n h√≥a n·ªôi dung chunk ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n
                normalized_chunk = self._normalize_vietnamese_text(chunk)
                
                # Metadata cho m·ªói chunk
                # filename_uuid = file_id g·ªëc ƒë·ªÉ c√≥ th·ªÉ nh√≥m t·∫•t c·∫£ chunks c·ªßa c√πng 1 file
                chunk_metadata = {
                    "file_id": file_id,
                    "chunk_index": i,
                    "title": title,
                    "description": description,
                    "filename": metadata.get("original_filename", ""),
                    "filename_uuid": file_id,  # D√πng file_id g·ªëc l√†m filename_uuid ƒë·ªÉ nh√≥m chunks theo file
                    "upload_time": metadata.get("upload_time", ""),
                    "file_size": metadata.get("file_size", 0),
                    "pages_count": metadata.get("pages_count", 0),
                    "chunk_length": len(chunk),
                    "language": language,
                    "normalized_content": normalized_chunk  # Th√™m content ƒë√£ chu·∫©n h√≥a
                }
                
                chunk_ids.append(chunk_id)
                chunk_documents.append(chunk)
                chunk_metadatas.append(chunk_metadata)
            
            # L∆∞u v√†o ChromaDB v·ªõi auto-generated embeddings
            self.collection.add(
                documents=chunk_documents,
                metadatas=chunk_metadatas,
                ids=chunk_ids
            )
            
            print(f"‚úÖ Saved {len(text_chunks)} chunks to ChromaDB for file: {title}")
            return True, len(text_chunks), None
            
        except Exception as e:
            error_msg = f"Error saving to vector DB: {str(e)}"
            print(f"‚ùå {error_msg}")
            return False, 0, error_msg
    
    def search_in_vector_db(self, query, n_results=5, file_id=None):
        """
        T√¨m ki·∫øm trong vector database
        
        Args:
            query: C√¢u h·ªèi/t·ª´ kh√≥a t√¨m ki·∫øm
            n_results: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
            file_id: T√¨m ki·∫øm trong file c·ª• th·ªÉ (optional)
            
        Returns:
            tuple: (success, results, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # Chu·∫©n b·ªã filter n·∫øu c·∫ßn t√¨m trong file c·ª• th·ªÉ
            where_filter = None
            if file_id:
                where_filter = {"file_id": file_id}
            
            # Th·ª±c hi·ªán t√¨m ki·∫øm vector similarity
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=where_filter,
                include=["documents", "metadatas", "distances"]
            )
            
            # Format k·∫øt qu·∫£ tr·∫£ v·ªÅ
            formatted_results = []
            if results["documents"] and results["documents"][0]:
                for i, doc in enumerate(results["documents"][0]):
                    result_item = {
                        "content": doc,
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "similarity_score": 1 - results["distances"][0][i] if results["distances"] else 0
                    }
                    formatted_results.append(result_item)
            
            return True, formatted_results, None
            
        except Exception as e:
            error_msg = f"Error searching vector DB: {str(e)}"
            return False, [], error_msg
    
    def delete_from_vector_db(self, file_id):
        """
        X√≥a t·∫•t c·∫£ chunks c·ªßa m·ªôt file kh·ªèi vector database
        
        Args:
            file_id: UUID c·ªßa file c·∫ßn x√≥a
            
        Returns:
            tuple: (success, error_message)
        """
        try:
            if not self.collection:
                return False, "ChromaDB not initialized"
            
            # T√¨m t·∫•t c·∫£ chunks c·ªßa file n√†y
            results = self.collection.get(
                where={"file_id": file_id},
                include=["documents"]
            )
            
            if results["ids"]:
                # X√≥a t·∫•t c·∫£ chunks
                self.collection.delete(ids=results["ids"])
                print(f"‚úÖ Deleted {len(results['ids'])} chunks from ChromaDB for file: {file_id}")
            
            return True, None
            
        except Exception as e:
            error_msg = f"Error deleting from vector DB: {str(e)}"
            return False, error_msg
    
    def process_uploaded_file(self, file, title, description):
        """
        X·ª≠ l√Ω file upload ho√†n ch·ªânh
        
        Args:
            file: File object t·ª´ request
            title: Ti√™u ƒë·ªÅ t√†i li·ªáu
            description: M√¥ t·∫£ t√†i li·ªáu
            
        Returns:
            tuple: (success, result_data, error_message, status_code)
        """
        try:
            # Validate file
            if not file or not file.filename:
                return False, None, "No file selected", 400
            
            if not self.is_allowed_file(file.filename):
                return False, None, "Only PDF files are allowed", 400
            
            # Validate file size
            is_valid_size, file_size, size_error = self.validate_file_size(file)
            if not is_valid_size:
                return False, None, size_error, 413
            
            # Generate unique filename
            unique_filename, timestamp = self.generate_unique_filename(file.filename)
            file_path = os.path.join(self.upload_folder, unique_filename)
            
            # Save file
            file.save(file_path)
            
            # Extract text from PDF
            extract_success, extracted_text, pages_count, extract_error = self.extract_text_from_pdf(file_path)
            if not extract_success:
                # Clean up file if extraction failed
                if os.path.exists(file_path):
                    os.remove(file_path)
                return False, None, extract_error, 500
            
            # Calculate file hash
            file_hash = self.calculate_file_hash(file_path)
            
            # Generate UUID for file_id
            file_id = str(uuid.uuid4())
            
            # Create metadata
            metadata = {
                "file_id": file_id,
                "original_filename": file.filename,
                "title": title,
                "stored_filename": unique_filename,
                "file_path": file_path,
                "file_size": file_size,
                "file_hash": file_hash,
                "pages_count": pages_count,
                "text_length": len(extracted_text),
                "upload_time": datetime.now().isoformat(),
                "description": description,
                "extracted_text": extracted_text
            }
            
            # Save metadata
            metadata_success, metadata_path, metadata_error = self.save_file_metadata(file_id, metadata)
            if not metadata_success:
                # Clean up file if metadata saving failed
                if os.path.exists(file_path):
                    os.remove(file_path)
                return False, None, metadata_error, 500
            
            # Save extracted text
            text_success, text_path, text_error = self.save_extracted_text(file_id, extracted_text)
            if not text_success:
                # Clean up files if text saving failed
                if os.path.exists(file_path):
                    os.remove(file_path)
                if os.path.exists(metadata_path):
                    os.remove(metadata_path)
                return False, None, text_error, 500
            
            # L∆∞u v√†o ChromaDB vector database
            # Ch·ª©c nƒÉng n√†y cho ph√©p t√¨m ki·∫øm semantic trong n·ªôi dung PDF
            vector_success, chunks_count, vector_error = self.save_to_vector_db(
                file_id, title, description, extracted_text, metadata
            )
            
            # Ghi log nh∆∞ng kh√¥ng fail n·∫øu vector DB c√≥ l·ªói
            if not vector_success:
                print(f"‚ö†Ô∏è Warning: Could not save to vector DB: {vector_error}")
                # Kh√¥ng return error v√¨ file ƒë√£ ƒë∆∞·ª£c l∆∞u th√†nh c√¥ng
            else:
                print(f"‚úÖ Successfully saved {chunks_count} text chunks to vector database")
            
            # Prepare response data (exclude sensitive info)
            response_data = {
                key: value for key, value in metadata.items() 
                if key not in ['extracted_text', 'file_path', 'stored_filename']
            }
            
            # Th√™m th√¥ng tin v·ªÅ vector DB v√†o response
            response_data['vector_chunks_count'] = chunks_count if vector_success else 0
            response_data['vector_db_status'] = 'success' if vector_success else 'warning'
            
            return True, response_data, "File uploaded and processed successfully", 200
            
        except Exception as e:
            return False, None, f"Failed to process file: {str(e)}", 500
    
    def get_uploaded_files(self):
        """
        L·∫•y danh s√°ch c√°c file ƒë√£ upload
        
        Returns:
            tuple: (success, files_data, error_message)
        """
        try:
            files_list = []
            
            # Ki·ªÉm tra th∆∞ m·ª•c upload t·ªìn t·∫°i
            if not os.path.exists(self.upload_folder):
                return True, {"files": [], "total_files": 0}, None
            
            # Duy·ªát qua c√°c file metadata
            for filename in os.listdir(self.upload_folder):
                if filename.endswith('_metadata.json'):
                    metadata_path = os.path.join(self.upload_folder, filename)
                    try:
                        with open(metadata_path, 'r', encoding='utf-8') as f:
                            metadata = json.load(f)
                        
                        # Ch·ªâ l·∫•y th√¥ng tin c·∫ßn thi·∫øt
                        file_info = {
                            'file_id': metadata.get('file_id'),
                            'filename': metadata.get('original_filename'),
                            'title': metadata.get('title'),
                            'file_size': metadata.get('file_size'),
                            'pages_count': metadata.get('pages_count'),
                            'text_length': metadata.get('text_length'),
                            'upload_time': metadata.get('upload_time'),
                            'description': metadata.get('description', '')
                        }
                        files_list.append(file_info)
                        
                    except Exception as e:
                        # Skip files with invalid metadata
                        continue
            
            # S·∫Øp x·∫øp theo th·ªùi gian upload (m·ªõi nh·∫•t tr∆∞·ªõc)
            files_list.sort(key=lambda x: x.get('upload_time', ''), reverse=True)
            
            result_data = {
                "files": files_list,
                "total_files": len(files_list)
            }
            
            return True, result_data, None
            
        except Exception as e:
            return False, None, f"Failed to list files: {str(e)}"
    
    def get_file_by_id(self, file_id):
        """
        L·∫•y th√¥ng tin chi ti·∫øt c·ªßa m·ªôt file theo ID
        
        Args:
            file_id: ID c·ªßa file
            
        Returns:
            tuple: (success, file_data, error_message)
        """
        try:
            metadata_path = os.path.join(self.upload_folder, f"{file_id}_metadata.json")
            
            if not os.path.exists(metadata_path):
                return False, None, "File not found"
            
            with open(metadata_path, 'r', encoding='utf-8') as f:
                metadata = json.load(f)
            
            # Exclude sensitive information
            file_data = {
                key: value for key, value in metadata.items() 
                if key not in ['file_path', 'stored_filename']
            }
            
            return True, file_data, None
            
        except Exception as e:
            return False, None, f"Error retrieving file: {str(e)}"
    
    def delete_file(self, file_id):
        """
        X√≥a file v√† metadata
        
        Args:
            file_id: ID c·ªßa file
            
        Returns:
            tuple: (success, error_message)
        """
        try:
            # Paths to delete
            metadata_path = os.path.join(self.upload_folder, f"{file_id}_metadata.json")
            text_path = os.path.join(self.upload_folder, f"{file_id}_text.txt")
            
            # Get file path from metadata
            if os.path.exists(metadata_path):
                with open(metadata_path, 'r', encoding='utf-8') as f:
                    metadata = json.load(f)
                    file_path = metadata.get('file_path')
            else:
                return False, "File not found"
            
            # X√≥a kh·ªèi vector database tr∆∞·ªõc
            # ƒêi·ªÅu n√†y ƒë·∫£m b·∫£o kh√¥ng c√≤n tham chi·∫øu ƒë·∫øn file trong vector DB
            vector_success, vector_error = self.delete_from_vector_db(file_id)
            if not vector_success:
                print(f"‚ö†Ô∏è Warning: Could not delete from vector DB: {vector_error}")
                # Ti·∫øp t·ª•c x√≥a file d√π vector DB c√≥ l·ªói
            
            # Delete files from filesystem
            files_to_delete = [metadata_path, text_path]
            if file_path and os.path.exists(file_path):
                files_to_delete.append(file_path)
            
            for file_to_delete in files_to_delete:
                if os.path.exists(file_to_delete):
                    os.remove(file_to_delete)
            
            print(f"‚úÖ Successfully deleted file {file_id} and all associated data")
            return True, None
            
        except Exception as e:
            return False, f"Error deleting file: {str(e)}"
    
    def search_knowledge_base(self, query, max_results=5, file_id=None):
        """
        T√¨m ki·∫øm trong knowledge base s·ª≠ d·ª•ng vector similarity
        
        Args:
            query: C√¢u h·ªèi ho·∫∑c t·ª´ kh√≥a t√¨m ki·∫øm
            max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa tr·∫£ v·ªÅ
            file_id: T√¨m ki·∫øm trong file c·ª• th·ªÉ (optional)
            
        Returns:
            tuple: (success, search_results, error_message)
        """
        try:
            # T√¨m ki·∫øm trong vector database
            vector_success, vector_results, vector_error = self.search_in_vector_db(
                query, max_results, file_id
            )
            
            if not vector_success:
                return False, [], f"Vector search failed: {vector_error}"
            
            # Format k·∫øt qu·∫£ cho API response
            formatted_results = []
            for result in vector_results:
                formatted_result = {
                    "content": result["content"],
                    "similarity_score": round(result["similarity_score"], 4),
                    "source": {
                        "file_id": result["metadata"].get("file_id"),
                        "title": result["metadata"].get("title"),
                        "filename": result["metadata"].get("filename"),
                        "filename_uuid": result["metadata"].get("filename_uuid"),
                        "chunk_index": result["metadata"].get("chunk_index")
                    }
                }
                formatted_results.append(formatted_result)
            
            return True, formatted_results, None
            
        except Exception as e:
            return False, [], f"Search failed: {str(e)}"
    
    def get_vector_db_stats(self):
        """
        L·∫•y th·ªëng k√™ v·ªÅ vector database
        
        Returns:
            tuple: (success, stats, error_message)
        """
        try:
            if not self.collection:
                return False, {}, "ChromaDB not initialized"
            
            # L·∫•y s·ªë l∆∞·ª£ng documents trong collection
            count = self.collection.count()
            
            # L·∫•y th√¥ng tin v·ªÅ collection
            stats = {
                "total_chunks": count,
                "collection_name": self.collection.name,
                "db_path": self.chroma_db_path
            }
            
            return True, stats, None
            
        except Exception as e:
            return False, {}, f"Error getting stats: {str(e)}"
    
    # =============================================================================
    # TRUY XU·∫§T D·ªÆ LI·ªÜU T·ª™ CHROMADB
    # =============================================================================
    
    def get_all_chunks(self, limit=None):
        """
        L·∫•y t·∫•t c·∫£ chunks t·ª´ ChromaDB (kh√¥ng c·∫ßn search query)
        
        Args:
            limit: Gi·ªõi h·∫°n s·ªë l∆∞·ª£ng chunks tr·∫£ v·ªÅ (optional)
            
        Returns:
            tuple: (success, chunks_data, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # L·∫•y t·∫•t c·∫£ documents trong collection
            results = self.collection.get(
                limit=limit,
                include=["documents", "metadatas"]
            )
            
            # Format k·∫øt qu·∫£
            chunks_data = []
            if results["documents"]:
                for i, doc in enumerate(results["documents"]):
                    chunk_info = {
                        "id": results["ids"][i],
                        "content": doc,
                        "metadata": results["metadatas"][i] if results["metadatas"] else {}
                    }
                    chunks_data.append(chunk_info)
            
            return True, chunks_data, None
            
        except Exception as e:
            return False, [], f"Error getting all chunks: {str(e)}"
    
    def get_chunks_by_file_id(self, file_id):
        """
        L·∫•y t·∫•t c·∫£ chunks c·ªßa m·ªôt file c·ª• th·ªÉ
        
        Args:
            file_id: UUID c·ªßa file
            
        Returns:
            tuple: (success, chunks_data, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # T√¨m t·∫•t c·∫£ chunks c√≥ file_id c·ª• th·ªÉ
            results = self.collection.get(
                where={"file_id": file_id},
                include=["documents", "metadatas"]
            )
            
            # S·∫Øp x·∫øp theo chunk_index
            chunks_data = []
            if results["documents"]:
                for i, doc in enumerate(results["documents"]):
                    chunk_info = {
                        "id": results["ids"][i],
                        "content": doc,
                        "metadata": results["metadatas"][i] if results["metadatas"] else {},
                        "chunk_index": results["metadatas"][i].get("chunk_index", 0) if results["metadatas"] else 0
                    }
                    chunks_data.append(chunk_info)
                
                # S·∫Øp x·∫øp theo chunk_index
                chunks_data.sort(key=lambda x: x["chunk_index"])
            
            return True, chunks_data, None
            
        except Exception as e:
            return False, [], f"Error getting chunks by file ID: {str(e)}"
    
    def get_chunks_by_title(self, title):
        """
        L·∫•y chunks theo title c·ªßa document
        
        Args:
            title: Ti√™u ƒë·ªÅ document
            
        Returns:
            tuple: (success, chunks_data, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # T√¨m chunks c√≥ title c·ª• th·ªÉ
            results = self.collection.get(
                where={"title": title},
                include=["documents", "metadatas"]
            )
            
            chunks_data = []
            if results["documents"]:
                for i, doc in enumerate(results["documents"]):
                    chunk_info = {
                        "id": results["ids"][i],
                        "content": doc,
                        "metadata": results["metadatas"][i] if results["metadatas"] else {}
                    }
                    chunks_data.append(chunk_info)
            
            return True, chunks_data, None
            
        except Exception as e:
            return False, [], f"Error getting chunks by title: {str(e)}"
    
    def get_chunk_by_id(self, chunk_id):
        """
        L·∫•y m·ªôt chunk c·ª• th·ªÉ theo ID
        
        Args:
            chunk_id: ID c·ªßa chunk (format: file_id_chunk_index)
            
        Returns:
            tuple: (success, chunk_data, error_message)
        """
        try:
            if not self.collection:
                return False, None, "ChromaDB not initialized"
            
            # L·∫•y chunk theo ID
            results = self.collection.get(
                ids=[chunk_id],
                include=["documents", "metadatas"]
            )
            
            if results["documents"] and len(results["documents"]) > 0:
                chunk_data = {
                    "id": chunk_id,
                    "content": results["documents"][0],
                    "metadata": results["metadatas"][0] if results["metadatas"] else {}
                }
                return True, chunk_data, None
            else:
                return False, None, "Chunk not found"
            
        except Exception as e:
            return False, None, f"Error getting chunk by ID: {str(e)}"
    
    def filter_chunks_by_metadata(self, filters, limit=None):
        """
        L·ªçc chunks theo metadata
        
        Args:
            filters: Dictionary ch·ª©a ƒëi·ªÅu ki·ªán l·ªçc
                    V√≠ d·ª•: {"file_size": {"$gte": 1000}, "pages_count": {"$lte": 10}}
            limit: Gi·ªõi h·∫°n s·ªë k·∫øt qu·∫£
            
        Returns:
            tuple: (success, chunks_data, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # L·ªçc chunks theo metadata
            results = self.collection.get(
                where=filters,
                limit=limit,
                include=["documents", "metadatas"]
            )
            
            chunks_data = []
            if results["documents"]:
                for i, doc in enumerate(results["documents"]):
                    chunk_info = {
                        "id": results["ids"][i],
                        "content": doc,
                        "metadata": results["metadatas"][i] if results["metadatas"] else {}
                    }
                    chunks_data.append(chunk_info)
            
            return True, chunks_data, None
            
        except Exception as e:
            return False, [], f"Error filtering chunks: {str(e)}"
    
    def get_files_summary_from_chunks(self):
        """
        L·∫•y t√≥m t·∫Øt th√¥ng tin c√°c file t·ª´ chunks trong ChromaDB
        
        Returns:
            tuple: (success, files_summary, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # L·∫•y t·∫•t c·∫£ chunks
            results = self.collection.get(include=["metadatas"])
            
            if not results["metadatas"]:
                return True, [], None
            
            # Nh√≥m chunks theo file_id
            files_dict = {}
            for metadata in results["metadatas"]:
                file_id = metadata.get("file_id")
                if file_id not in files_dict:
                    files_dict[file_id] = {
                        "file_id": file_id,
                        "title": metadata.get("title", ""),
                        "filename": metadata.get("filename", ""),
                        "upload_time": metadata.get("upload_time", ""),
                        "file_size": metadata.get("file_size", 0),
                        "pages_count": metadata.get("pages_count", 0),
                        "chunks_count": 0,
                        "total_chunk_length": 0
                    }
                
                # C·∫≠p nh·∫≠t th·ªëng k√™
                files_dict[file_id]["chunks_count"] += 1
                files_dict[file_id]["total_chunk_length"] += metadata.get("chunk_length", 0)
            
            # Chuy·ªÉn th√†nh list v√† s·∫Øp x·∫øp
            files_summary = list(files_dict.values())
            files_summary.sort(key=lambda x: x["upload_time"], reverse=True)
            
            return True, files_summary, None
            
        except Exception as e:
            return False, [], f"Error getting files summary: {str(e)}"
    
    def search_chunks_with_filters(self, query, filters=None, n_results=5):
        """
        T√¨m ki·∫øm vector similarity v·ªõi filters metadata
        
        Args:
            query: C√¢u h·ªèi t√¨m ki·∫øm
            filters: ƒêi·ªÅu ki·ªán l·ªçc metadata (optional)
            n_results: S·ªë k·∫øt qu·∫£ tr·∫£ v·ªÅ
            
        Returns:
            tuple: (success, results, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            # Th·ª±c hi·ªán t√¨m ki·∫øm v·ªõi filters
            results = self.collection.query(
                query_texts=[query],
                n_results=n_results,
                where=filters,
                include=["documents", "metadatas", "distances"]
            )
            
            # Format k·∫øt qu·∫£
            formatted_results = []
            if results["documents"] and results["documents"][0]:
                for i, doc in enumerate(results["documents"][0]):
                    result_item = {
                        "content": doc,
                        "metadata": results["metadatas"][0][i] if results["metadatas"] else {},
                        "similarity_score": 1 - results["distances"][0][i] if results["distances"] else 0
                    }
                    formatted_results.append(result_item)
            
            return True, formatted_results, None
            
        except Exception as e:
            return False, [], f"Error searching with filters: {str(e)}"

    def search_in_multiple_files(self, query, filename_uuids, max_results=5):
        """
        T√¨m ki·∫øm trong nhi·ªÅu files c·ª• th·ªÉ d·ª±a tr√™n list filename_uuid
        H·ªó tr·ª£ t√¨m ki·∫øm ti·∫øng Vi·ªát v·ªõi x·ª≠ l√Ω vƒÉn b·∫£n c·∫£i ti·∫øn
        
        Args:
            query: C√¢u h·ªèi t√¨m ki·∫øm (ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh)
            filename_uuids: List c√°c filename_uuid ƒë·ªÉ t√¨m ki·∫øm
            max_results: S·ªë k·∫øt qu·∫£ t·ªëi ƒëa tr·∫£ v·ªÅ
            
        Returns:
            tuple: (success, results, error_message)
        """
        try:
            if not self.collection:
                return False, [], "ChromaDB not initialized"
            
            if not filename_uuids or len(filename_uuids) == 0:
                return False, [], "No filename_uuids provided"
            
            # T·∫°o filter ƒë·ªÉ t√¨m ki·∫øm trong c√°c files c·ª• th·ªÉ
            filters = {
                "filename_uuid": {"$in": filename_uuids}
            }
            
            print(f"üîç Searching for query: '{query}' in files: {filename_uuids}")
            
            # Chi·∫øn l∆∞·ª£c 1: T√¨m ki·∫øm tr·ª±c ti·∫øp v·ªõi query g·ªëc
            success, search_results, error_message = self.search_chunks_with_filters(
                query, filters, max_results
            )
            
            if success and len(search_results) > 0:
                print(f"‚úÖ Found {len(search_results)} results with original query")
                return self._format_search_results(search_results, query, filename_uuids, max_results)
            
            # Chi·∫øn l∆∞·ª£c 2: Chu·∫©n h√≥a query v√† th·ª≠ l·∫°i
            normalized_query = self._normalize_vietnamese_text(query)
            if normalized_query != query:
                print(f"üîÑ Trying normalized query: '{normalized_query}'")
                success, search_results, error_message = self.search_chunks_with_filters(
                    normalized_query, filters, max_results
                )
                
                if success and len(search_results) > 0:
                    print(f"‚úÖ Found {len(search_results)} results with normalized query")
                    return self._format_search_results(search_results, query, filename_uuids, max_results)
            
            # Chi·∫øn l∆∞·ª£c 3: T√¨m ki·∫øm v·ªõi t·ª´ kh√≥a
            keywords = self._extract_keywords_vietnamese(query)
            if keywords:
                keyword_query = " ".join(keywords)
                print(f"üîë Trying keyword search: '{keyword_query}'")
                success, search_results, error_message = self.search_chunks_with_filters(
                    keyword_query, filters, max_results
                )
                
                if success and len(search_results) > 0:
                    print(f"‚úÖ Found {len(search_results)} results with keyword search")
                    return self._format_search_results(search_results, query, filename_uuids, max_results)
            
            # Chi·∫øn l∆∞·ª£c 4: T√¨m ki·∫øm text matching tr·ª±c ti·∫øp
            print("üîç Trying direct text matching...")
            text_match_results = self._search_text_matching(query, filename_uuids, max_results)
            if text_match_results:
                print(f"‚úÖ Found {len(text_match_results)} results with text matching")
                return True, text_match_results, f"Found {len(text_match_results)} results using text matching"
            
            # N·∫øu kh√¥ng t√¨m th·∫•y g√¨
            if not success:
                return False, [], error_message
            
            return True, [], "No matching results found"
            
        except Exception as e:
            return False, [], f"Error searching in multiple files: {str(e)}"
    
    def _search_text_matching(self, query, filename_uuids, max_results=5):
        """
        T√¨m ki·∫øm b·∫±ng text matching tr·ª±c ti·∫øp (kh√¥ng d√πng vector similarity)
        H·ªØu √≠ch cho c√°c t·ª´ kh√≥a c·ª• th·ªÉ ho·∫∑c khi vector search kh√¥ng hi·ªáu qu·∫£
        """
        try:
            # L·∫•y t·∫•t c·∫£ chunks c·ªßa c√°c files ƒë∆∞·ª£c ch·ªâ ƒë·ªãnh
            all_chunks = self.collection.get(
                where={"filename_uuid": {"$in": filename_uuids}},
                include=["documents", "metadatas"]
            )
            
            if not all_chunks["documents"]:
                return []
            
            # Chu·∫©n h√≥a query ƒë·ªÉ so s√°nh
            normalized_query = self._normalize_vietnamese_text(query).lower()
            query_keywords = set(normalized_query.split())
            
            # T√¨m ki·∫øm text matching
            matching_results = []
            for i, doc in enumerate(all_chunks["documents"]):
                if not doc:
                    continue
                
                normalized_doc = self._normalize_vietnamese_text(doc).lower()
                doc_words = set(normalized_doc.split())
                
                # T√≠nh ƒëi·ªÉm d·ª±a tr√™n s·ªë t·ª´ kh√≥a kh·ªõp
                matching_words = query_keywords.intersection(doc_words)
                if matching_words:
                    score = len(matching_words) / len(query_keywords)
                    
                    # Ki·ªÉm tra xem c√≥ ch·ª©a phrase kh√¥ng
                    if normalized_query in normalized_doc:
                        score += 0.5  # Bonus cho exact phrase match
                    
                    metadata = all_chunks["metadatas"][i] if i < len(all_chunks["metadatas"]) else {}
                    
                    result = {
                        "content": doc,
                        "similarity_score": min(score, 1.0),  # Cap at 1.0
                        "metadata": metadata,
                        "matching_words": list(matching_words)
                    }
                    matching_results.append(result)
            
            # S·∫Øp x·∫øp theo ƒëi·ªÉm s·ªë v√† tr·∫£ v·ªÅ top results
            matching_results.sort(key=lambda x: x["similarity_score"], reverse=True)
            
            # Format results gi·ªëng nh∆∞ vector search
            formatted_results = []
            for result in matching_results[:max_results]:
                metadata = result.get("metadata", {})
                formatted_result = {
                    "content": result.get("content", ""),
                    "similarity_score": result.get("similarity_score", 0),
                    "source": {
                        "file_id": metadata.get("filename_uuid", ""),
                        "filename_uuid": metadata.get("filename_uuid", ""),
                        "title": metadata.get("title", ""),
                        "filename": metadata.get("filename", ""),
                        "chunk_index": metadata.get("chunk_index", 0),
                        "chunk_length": len(result.get("content", "")),
                        "search_method": "text_matching",
                        "matching_words": result.get("matching_words", [])
                    }
                }
                formatted_results.append(formatted_result)
            
            return formatted_results
            
        except Exception as e:
            print(f"Error in text matching search: {str(e)}")
            return []
    
    def _format_search_results(self, search_results, original_query, filename_uuids, max_results):
        """
        Format search results cho API response
        """
        formatted_results = []
        for result in search_results:
            metadata = result.get("metadata", {})
            formatted_result = {
                "content": result.get("content", ""),
                "similarity_score": result.get("similarity_score", 0),
                "source": {
                    "file_id": metadata.get("filename_uuid", ""),
                    "filename_uuid": metadata.get("filename_uuid", ""),
                    "title": metadata.get("title", ""),
                    "filename": metadata.get("filename", ""),
                    "chunk_index": metadata.get("chunk_index", 0),
                    "chunk_length": len(result.get("content", "")),
                    "search_method": "vector_similarity"
                }
            }
            formatted_results.append(formatted_result)
        
        return True, formatted_results, f"Found {len(formatted_results)} results in {len(filename_uuids)} files"
    
    def _normalize_vietnamese_text(self, text):
        """
        Chu·∫©n h√≥a text ti·∫øng Vi·ªát ƒë·ªÉ t√¨m ki·∫øm t·ªët h∆°n
        
        Args:
            text: Text c·∫ßn chu·∫©n h√≥a
            
        Returns:
            str: Text ƒë√£ ƒë∆∞·ª£c chu·∫©n h√≥a
        """
        if not text:
            return text
        
        # Chuy·ªÉn v·ªÅ lowercase
        normalized = text.lower()
        
        # Lo·∫°i b·ªè d·∫•u c√¢u th·ª´a
        normalized = re.sub(r'[^\w\s√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë]', ' ', normalized)
        
        # Lo·∫°i b·ªè multiple spaces
        normalized = re.sub(r'\s+', ' ', normalized).strip()
        
        return normalized
    
    def _extract_keywords_vietnamese(self, text):
        """
        Tr√≠ch xu·∫•t t·ª´ kh√≥a quan tr·ªçng t·ª´ query ti·∫øng Vi·ªát
        
        Args:
            text: Text c·∫ßn tr√≠ch xu·∫•t t·ª´ kh√≥a
            
        Returns:
            list: Danh s√°ch t·ª´ kh√≥a quan tr·ªçng
        """
        if not text:
            return []
        
        # Chu·∫©n h√≥a text
        normalized = self._normalize_vietnamese_text(text)
        
        # Danh s√°ch stop words ti·∫øng Vi·ªát ph·ªï bi·∫øn
        vietnamese_stop_words = {
            'l√†', 'c·ªßa', 'v√†', 'c√≥', 'trong', 'v·ªõi', 'ƒë·ªÉ', 'ƒë∆∞·ª£c', 'm·ªôt', 'c√°c', 'n√†y', 'ƒë√≥', 
            'nh∆∞', 'v·ªÅ', 'cho', 't·ª´', 'khi', 'n√†o', 'n·∫øu', 'th√¨', 's·∫Ω', 'ƒë√£', 'ƒëang', 'sao',
            'g√¨', 'ai', 'ƒë√¢u', 'bao', 'gi·ªù', 'n√†o', 'th·∫ø', 't·∫°i', 'v√¨', 'do', 'b·ªüi', 'theo',
            'the', 'a', 'an', 'and', 'or', 'but', 'in', 'on', 'at', 'to', 'for', 'of', 'with',
            'by', 'from', 'up', 'about', 'into', 'through', 'during', 'before', 'after', 'above',
            'below', 'between', 'among', 'is', 'are', 'was', 'were', 'be', 'been', 'being',
            'have', 'has', 'had', 'do', 'does', 'did', 'will', 'would', 'could', 'should'
        }
        
        # T√°ch t·ª´ v√† lo·∫°i b·ªè stop words
        words = normalized.split()
        keywords = []
        
        for word in words:
            # Gi·ªØ l·∫°i t·ª´ c√≥ ƒë·ªô d√†i >= 3 v√† kh√¥ng ph·∫£i stop word
            if len(word) >= 3 and word not in vietnamese_stop_words:
                keywords.append(word)
        
        # Gi·ªõi h·∫°n s·ªë t·ª´ kh√≥a tr·∫£ v·ªÅ
        return keywords[:5]
    
    def _detect_language(self, text):
        """
        Ph√°t hi·ªán ng√¥n ng·ªØ ch√≠nh c·ªßa text (ti·∫øng Vi·ªát ho·∫∑c ti·∫øng Anh)
        
        Args:
            text: Text c·∫ßn ph√¢n t√≠ch
            
        Returns:
            str: 'vi' cho ti·∫øng Vi·ªát, 'en' cho ti·∫øng Anh, 'mixed' cho h·ªón h·ª£p
        """
        if not text or len(text.strip()) < 10:
            return 'unknown'
        
        # C√°c k√Ω t·ª± ƒë·∫∑c tr∆∞ng ti·∫øng Vi·ªát
        vietnamese_chars = '√†√°·∫£√£·∫°ƒÉ·∫Ø·∫±·∫≥·∫µ·∫∑√¢·∫•·∫ß·∫©·∫´·∫≠√®√©·∫ª·∫Ω·∫π√™·∫ø·ªÅ·ªÉ·ªÖ·ªá√¨√≠·ªâƒ©·ªã√≤√≥·ªè√µ·ªç√¥·ªë·ªì·ªï·ªó·ªô∆°·ªõ·ªù·ªü·ª°·ª£√π√∫·ªß≈©·ª•∆∞·ª©·ª´·ª≠·ªØ·ª±·ª≥√Ω·ª∑·ªπ·ªµƒë'
        
        # ƒê·∫øm k√Ω t·ª± ti·∫øng Vi·ªát
        vietnamese_count = 0
        total_chars = 0
        
        for char in text.lower():
            if char.isalpha():
                total_chars += 1
                if char in vietnamese_chars:
                    vietnamese_count += 1
        
        if total_chars == 0:
            return 'unknown'
        
        vietnamese_ratio = vietnamese_count / total_chars
        
        # N·∫øu > 5% k√Ω t·ª± l√† ti·∫øng Vi·ªát th√¨ coi l√† ti·∫øng Vi·ªát
        if vietnamese_ratio > 0.05:
            return 'vi'
        elif vietnamese_ratio > 0.01:
            return 'mixed'
        else:
            return 'en'
    
    def debug_chunks_content(self, filename_uuids=None, limit=3):
        """
        Debug method ƒë·ªÉ xem n·ªôi dung th·ª±c t·∫ø c·ªßa chunks
        Gi√∫p hi·ªÉu t·∫°i sao t√¨m ki·∫øm kh√¥ng tr·∫£ v·ªÅ k·∫øt qu·∫£ mong mu·ªën
        
        Args:
            filename_uuids: List filename_uuid ƒë·ªÉ debug (optional)
            limit: S·ªë chunks t·ªëi ƒëa ƒë·ªÉ hi·ªÉn th·ªã
            
        Returns:
            tuple: (success, debug_info, error_message)
        """
        try:
            if not self.collection:
                return False, {}, "ChromaDB not initialized"
            
            # T·∫°o filter n·∫øu c√≥ filename_uuids
            where_filter = None
            if filename_uuids:
                where_filter = {"filename_uuid": {"$in": filename_uuids}}
            
            # L·∫•y chunks ƒë·ªÉ debug
            results = self.collection.get(
                where=where_filter,
                limit=limit,
                include=["documents", "metadatas"]
            )
            
            debug_info = {
                "total_chunks": len(results["documents"]) if results["documents"] else 0,
                "chunks_preview": []
            }
            
            if results["documents"]:
                for i, doc in enumerate(results["documents"][:limit]):
                    metadata = results["metadatas"][i] if i < len(results["metadatas"]) else {}
                    
                    chunk_info = {
                        "chunk_index": i,
                        "content_preview": doc[:200] + "..." if len(doc) > 200 else doc,
                        "content_length": len(doc),
                        "language": metadata.get("language", "unknown"),
                        "title": metadata.get("title", ""),
                        "filename": metadata.get("filename", ""),
                        "filename_uuid": metadata.get("filename_uuid", ""),
                        "chunk_metadata_index": metadata.get("chunk_index", 0)
                    }
                    debug_info["chunks_preview"].append(chunk_info)
            
            return True, debug_info, None
            
        except Exception as e:
            return False, {}, f"Error debugging chunks: {str(e)}"
    
    def reset_chromadb(self, confirm_reset=False):
        """
        Reset ChromaDB - X√≥a t·∫•t c·∫£ chunks v√† t·∫°o l·∫°i collection m·ªõi
        
        Args:
            confirm_reset: X√°c nh·∫≠n reset (b·∫£o v·ªá kh·ªèi x√≥a nh·∫ßm)
            
        Returns:
            tuple: (success, reset_info, error_message)
        """
        try:
            if not confirm_reset:
                return False, {}, "Please set confirm_reset=True to confirm reset operation"
            
            if not self.chroma_client:
                return False, {}, "ChromaDB client not initialized"
            
            # L·∫•y th√¥ng tin tr∆∞·ªõc khi reset
            old_collection_info = {}
            if self.collection:
                try:
                    old_data = self.collection.get(include=["documents", "metadatas"])
                    old_collection_info = {
                        "total_chunks_deleted": len(old_data["documents"]) if old_data["documents"] else 0,
                        "collection_name": "knowledge_base"
                    }
                except:
                    old_collection_info = {"total_chunks_deleted": "unknown", "collection_name": "knowledge_base"}
            
            # X√≥a collection c≈©
            try:
                self.chroma_client.delete_collection(name="knowledge_base")
                print("‚úÖ Deleted old collection 'knowledge_base'")
            except Exception as e:
                print(f"‚ö†Ô∏è Could not delete old collection (might not exist): {str(e)}")
            
            # T·∫°o collection m·ªõi
            self.collection = self.chroma_client.get_or_create_collection(
                name="knowledge_base",
                metadata={"description": "PDF document knowledge base with text chunks - Reset on " + datetime.now().isoformat()}
            )
            
            reset_info = {
                "reset_timestamp": datetime.now().isoformat(),
                "old_collection_info": old_collection_info,
                "new_collection_created": True,
                "db_path": self.chroma_db_path
            }
            
            print(f"üîÑ ChromaDB reset completed successfully")
            print(f"üìä Deleted {old_collection_info.get('total_chunks_deleted', 0)} chunks")
            
            return True, reset_info, "ChromaDB reset completed successfully"
            
        except Exception as e:
            return False, {}, f"Error resetting ChromaDB: {str(e)}"
    
    def clear_all_chunks(self):
        """
        X√≥a t·∫•t c·∫£ chunks nh∆∞ng gi·ªØ nguy√™n collection
        (Ph∆∞∆°ng th·ª©c nh·∫π h∆°n reset_chromadb)
        
        Returns:
            tuple: (success, clear_info, error_message)
        """
        try:
            if not self.collection:
                return False, {}, "ChromaDB collection not initialized"
            
            # L·∫•y t·∫•t c·∫£ IDs
            all_data = self.collection.get(include=["documents"])
            total_chunks = len(all_data["ids"]) if all_data["ids"] else 0
            
            if total_chunks == 0:
                return True, {"chunks_cleared": 0, "message": "No chunks to clear"}, "No chunks found"
            
            # X√≥a t·∫•t c·∫£ chunks
            self.collection.delete(ids=all_data["ids"])
            
            clear_info = {
                "chunks_cleared": total_chunks,
                "clear_timestamp": datetime.now().isoformat(),
                "collection_name": "knowledge_base"
            }
            
            print(f"üóëÔ∏è Cleared {total_chunks} chunks from ChromaDB")
            
            return True, clear_info, f"Cleared {total_chunks} chunks successfully"
            
        except Exception as e:
            return False, {}, f"Error clearing chunks: {str(e)}"
